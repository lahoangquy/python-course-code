{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec is a widely used word embedding technique in Natural language processing (NLP) that is used to represent words as dense, continous-valued vectors ina high-dimensional space. The Word2vec algorithm was developed by researchers at Google and has proven to be highly effective in capturing sematic relationships between words and preserving their context in a distributed representation.\n",
        "\n",
        "The major goal of Word2vec is to create word embeddings that encode the meaning of words based ont heir distributional properties in a large corpus of text. It leverages the distributional hypothesis which posits that owrds that appear in similar contexts often have similar meaning. Word2Vec learns these embeeding through unsupervised learning using neural netwroks or other techniques.\n",
        "\n",
        "\n",
        "There are 2 major archtectures for Word2Vec:\n",
        "\n",
        "\n",
        "\n",
        "*   Continuous Bag of words (CBOW): In CBow, the model predicts the target word based on the surrounding context words. The context words are represented as one-hot vecotds and the model learns to predict the target word from these context vectors\n",
        "*   skip-gram: The model predicts the context words given the target word. It takes a target word as input and tries to predict the surrounding context words\n",
        "\n",
        "\n",
        "After training Word2vec on a large text corpus, the resulting word embedddings are dense and distributed in a way that words with similar meanings or contexts are located close to each other in the embedding space. This property makes Word2Vec embeddings useful for various NLP tasks, such as similarity comparison, word analogies and sematic relationship exploration.\n",
        "\n",
        "Word2vec has revolutionized many NLP applications and is often used as a starting point for more complex langauge models like GloVe (Global Vecors for Word Representation) and BERT (Bidirectional Encoder Representations from Transformers). These models build on the idea of distributed word embeedings and have contributed significantly to the advancements in NLP over the years.\n",
        "\n",
        "By representing words as a continuous vecotds. Word2vec allows NLP models to perform more efficient computations and generalize better in downstream tasks leading to enhanced performance in various langauge related applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "nnzdbhiB6tBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F28at1Rt6bx5"
      },
      "outputs": [],
      "source": []
    }
  ]
}