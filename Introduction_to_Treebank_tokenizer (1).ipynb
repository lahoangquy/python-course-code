{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Treebank tokenizer is a specific tokenizer used in Natural language Processing (NLP) that follows the conventions of the Penn Treebank. It is commonly used for toenization in various NLP tasks, especially in the context of syntactic analysis and part-of-speech tagging\n",
        "\n",
        "\n",
        "The treebank tokenizer is designed to split text into individual toekns (words, or punctuation) based on the grammatical conventions used in the Penn Treebank, which is widely used corpus of English langauge ext annotated with syntactic information. This tokenizer implements a set of rules to handle various cases of tokenization taking into account punctuation, hyphenated words and other linguistic complexities\n",
        "\n",
        "\n",
        "Key features of the Treebank tokenizer:\n",
        "\n",
        "\n",
        "\n",
        "*   handling Punctuation: The treebank tokenizer treats punctuation marks as separate tokens. For example, \"hello, NLP\" would be tokenized into [\"Hello\", \",\", \"NLP\"]\n",
        "\n",
        "*   Contractions: Contractions kike don't, can't and won't will be treated as seperate tokens. For instance, \"I don't like it\" would be tokenized into [\"I\",\"do\",\"n't\",\"like\",\"it\"]\n",
        "\n",
        "*   Hyphenated words are considered as individual tokens, for example, \"well-known\" would be tokenized as [\"well-known\"]\n",
        "*   Special cases: The Treebank tokenizer addresses certain special cases such as handling parenthesis and brackets as seperate tokens.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0HuC-chZ_67r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mqRsOfnm_2cL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05032e27-b521-4467-f822-f2c86b6edf7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'that',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'cost',\n",
              " 'more',\n",
              " 'than',\n",
              " '$',\n",
              " '3000.0']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the result above, this tokenizer primarily helps in analyzing each component in the text seperately. The I'm gets split into 2 components, namely the I, which corresponds to a noun phrase and the 'm which corresponds to a verb component. This split allows us to work on individual tokens that carry significant information that whould have been difficult to analyze and prase if it was a single token. Similarly, doesn't gets splits into does and n't helping to better parse and undestand the inherent semantics assocuiated with the n't which in dicates negation"
      ],
      "metadata": {
        "id": "Ee8qXewm-Yhz"
      }
    }
  ]
}