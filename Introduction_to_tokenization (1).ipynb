{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniation is a fundametal concept in Natural langauge Processing (NLP) which involves breaking down a text or document into individual units, known as tokens. These tokens are typically words, phrases, or symbols that constitute the building blocks of a language. Tokenization is an important preprocessing step in NLP as it lays the foundation for further analysis and understanding of the text.\n",
        "\n",
        "In the context of tokenization, a token can represent :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Words: tokenization breaks the text into indvidual words. For example, the sentence : \" Natural language processing is fun!\" would be tokenized into [\"Natural\", \"Language\",\"Processing\", \"is\", \"fun\", \"!\"]\n",
        "\n",
        "*   Phrases: Tokenization can capture phrases as individual units. For instance, the phrase \"New York City\" would be considered a single token than 3 seperate words.\n",
        "\n",
        "*   Symbols and punctuations: Tokenization treats symbols, punctuation marks and special characters as seperate tokens. For example, \"NLP, AI, and ML.\" would be tokenized as  \"NLP\", \",\" ,\"AI\" , \",\" , \"and\" , \"ML\",\".\"\n",
        "\n",
        "\n",
        "Why we need to know tokenization\n",
        "\n",
        "Tokenization is an important step in NLP for servarl reasons:\n",
        "\n",
        "\n",
        "\n",
        "*   Text segmentation: tokenization breaks down the text into smaller, mearning ful units, facilitating further naalysis and processing of the lanauge. It creates a structured representation of the text that can be easily manipulated by NLP algorithms\n",
        "*   Volcabulary Creating: Tokenizaion forms the basis for building a vocabulary or lexicon which is essential for langauge understanding and modeling. Each token becomes an entry in the vocabulary enabling the development of language models and statistical techniques.\n",
        "\n",
        "\n",
        "*   Text normalization: Tokenization helps in normalizing the text by converting all characters to lowercase or applying other text cleaning operations which aids in reducing vocabulary size and achieving consistency.\n",
        "\n",
        "*   Feature extraction: Tokens serve as features for various NLP tasks such as sentiment analysis, machine translation, and text classification. They represent the basic nuits on which NLP model operates.\n",
        "*   There are several tokenization techniques: Word Toeknizaiton: This technique breaks the text into individual setntences based on punctuation marks like periods, exclamation marks and question marks\n",
        "\n",
        "\n",
        "*   phrase tokenization: In addition to breaking the text into words, this technique identifies and preserves multi-word phrases that carry specific meaning like \"New York City\" or machine learning\n",
        "\n",
        "\n",
        "\n",
        "*   subword tokenization: This technique divides words into smaller subword units which is particularly useful for handling morphologically rich languages or dealing with rate words in limited-resource scenarios.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2vEIbYLT7YyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXph-V0k6rM8",
        "outputId": "cca0fdc0-4859-403a-a9b7-8e9c07904444"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'capital', 'of', 'Viet', 'Nam', 'is', 'Hanoi']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "sentence = \"The capital of Viet Nam is Hanoi\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple sentence.split() method could provide us with all the various tokens in the sentence of The capital of Viet Nam is Hanoi. Each token in the above split carries an instrinsic meaning, but it is not always as straightforward as this."
      ],
      "metadata": {
        "id": "RFovPA1AQsVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"VietNam's captial is Hanoi\""
      ],
      "metadata": {
        "id": "iUAZIG8xQ_3L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvK1EBZGRLed",
        "outputId": "dff96c8a-3f07-45e9-fdfb-098152018c08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"VietNam's\", 'captial', 'is', 'Hanoi']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example should it be VietNam, Vietnams or VietNam's . A split method does not often know how to deal with situatuions containg apostrophes."
      ],
      "metadata": {
        "id": "zlWtMiIMRThO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"VietNam is where we'll go\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kEt0jXwRmE4",
        "outputId": "ac9c649e-e249-4fea-8d88-ce52c525e39c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VietNam', 'is', 'where', \"we'll\", 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Should it be well, we'll or we and 'll seperattely. An ideal tokenizer should be about to process we'll into 2 tokens which are we and will"
      ],
      "metadata": {
        "id": "5HYt5YXnR159"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Let's travel Ha Noi in VietNam\""
      ],
      "metadata": {
        "id": "y87wFoVQSFt6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNrVV565SL7W",
        "outputId": "57283ffa-630c-45a4-8d5c-3e7fbaefb1e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'travel', 'Ha', 'Noi', 'in', 'VietNam']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here ideally Ha Noi should be 1 token."
      ],
      "metadata": {
        "id": "EZXKH4h5SPqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence  = \"Most of the times umm I want to travel\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFCunE95SgOR",
        "outputId": "5e61b0a9-0417-476e-d555-d875cef58492"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Most', 'of', 'the', 'times', 'umm', 'I', 'want', 'to', 'travel']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does a token such as umm carry any meaning? Should it be removed or should not. Even though a token such as umm is not a part of English vacabulary it becomes  important in use cases where speech synthesis is involved as it indicates that the person is taking a pause here and trying to think of something.  Again as well as the context, the notion of the use case also mtters when understanding where somehting should be tokenized or simply removed as a gragment of text that does not convey any meaning.\n",
        "\n",
        "The rise of social media platforms has resulted in a massive influx of user data, which is a rich mine of information to understand individuals and communities, but it has also caterd to the rise of a word of emotions, short forms, new abbreviations (often called millennial langauge) and so on. There is a need to understand this very-growing kind of text as well those cases where for instance, a character P used with a colon (:) and hyphen (-) denotes a face with a stuck -out tongue. Hashtags are another very common thing on social media that are mostly indicative of summaries or emotions behing a Facebook post or a tweet on Twitter"
      ],
      "metadata": {
        "id": "X5rx_EJCSlgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence= \"VietNam is a cool place!!!! :-P <3 #Awesome\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYdvvlYCUE8b",
        "outputId": "7413e4fd-dda3-410b-fc0c-465d4d76469b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VietNam', 'is', 'a', 'cool', 'place!!!!', ':-P', '<3', '#Awesome']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}