{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniation is a fundametal concept in Natural langauge Processing (NLP) which involves breaking down a text or document into individual units, known as tokens. These tokens are typically words, phrases, or symbols that constitute the building blocks of a language. Tokenization is an important preprocessing step in NLP as it lays the foundation for further analysis and understanding of the text.\n",
        "\n",
        "In the context of tokenization, a token can represent :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Words: tokenization breaks the text into indvidual words. For example, the sentence : \" Natural language processing is fun!\" would be tokenized into [\"Natural\", \"Language\",\"Processing\", \"is\", \"fun\", \"!\"]\n",
        "\n",
        "*   Phrases: Tokenization can capture phrases as individual units. For instance, the phrase \"New York City\" would be considered a single token than 3 seperate words.\n",
        "\n",
        "*   Symbols and punctuations: Tokenization treats symbols, punctuation marks and special characters as seperate tokens. For example, \"NLP, AI, and ML.\" would be tokenized as  \"NLP\", \",\" ,\"AI\" , \",\" , \"and\" , \"ML\",\".\"\n",
        "\n",
        "\n",
        "Why we need to know tokenization\n",
        "\n",
        "Tokenization is an important step in NLP for servarl reasons:\n",
        "\n",
        "\n",
        "\n",
        "*   Text segmentation: tokenization breaks down the text into smaller, mearning ful units, facilitating further naalysis and processing of the lanauge. It creates a structured representation of the text that can be easily manipulated by NLP algorithms\n",
        "*   Volcabulary Creating: Tokenizaion forms the basis for building a vocabulary or lexicon which is essential for langauge understanding and modeling. Each token becomes an entry in the vocabulary enabling the development of language models and statistical techniques.\n",
        "\n",
        "\n",
        "*   Text normalization: Tokenization helps in normalizing the text by converting all characters to lowercase or applying other text cleaning operations which aids in reducing vocabulary size and achieving consistency.\n",
        "\n",
        "*   Feature extraction: Tokens serve as features for various NLP tasks such as sentiment analysis, machine translation, and text classification. They represent the basic nuits on which NLP model operates.\n",
        "*   There are several tokenization techniques: Word Toeknizaiton: This technique breaks the text into individual setntences based on punctuation marks like periods, exclamation marks and question marks\n",
        "\n",
        "\n",
        "*   phrase tokenization: In addition to breaking the text into words, this technique identifies and preserves multi-word phrases that carry specific meaning like \"New York City\" or machine learning\n",
        "\n",
        "\n",
        "\n",
        "*   subword tokenization: This technique divides words into smaller subword units which is particularly useful for handling morphologically rich languages or dealing with rate words in limited-resource scenarios.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2vEIbYLT7YyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXph-V0k6rM8"
      },
      "outputs": [],
      "source": []
    }
  ]
}