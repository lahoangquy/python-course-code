{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 main types of categorires.\n",
        "\n",
        "1) Structured Data: This is the most organized form of data. It is represented in tabular formats such as Excel files and Comma-Separated Value (CSV) files.\n",
        "\n",
        "2) Semi-Structured Data: This type of data is not presented in a tabular structure, but it can be represented in a tabular format after transformation. Here, information is usually stored between tags following a definite pattern. XML and HTML files can be referred to as semi-structured data.\n",
        "\n",
        "3)Unstructured Data: This type of data is the most difficult to deal with. Machine learning algorithms would find it difficult to comprehend unstructured data without any loss of information. Text corpora and images are examples of unstructured data.\n",
        "\n",
        "Categorization of Data Based on Content:\n",
        "\n",
        "•\tText Data: This refers to text corpora consisting of written sentences. This type of data can only be read. An example would be the text corpus of a book.\n",
        "•\tImage Data: This refers to pictures that are used to communicate messages. This type of data can only be seen.\n",
        "•\tAudio Data: This refers to recordings of someone's voice, music, and so on. This type of data can only be heard.\n",
        "•\tVideo Data: A continuous series of images coupled with audio forms a video. This type of data can be seen as well as heard.\n",
        "\n"
      ],
      "metadata": {
        "id": "t4xawhC2O-Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "KyPy9wfBUpgq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oo25H8poOeB6"
      },
      "outputs": [],
      "source": [
        "sentence = 'Happy tweeted, \"Witnessing 90th Republic Day of Viet Nam from Jack, \\\n",
        "Ho CHi Minh. Mesmerizing performance by Vietnamese Army! Awesome airshow! @Viet_Nam_official \\\n",
        "@Viet_Nam_official #VietNam #90thRepublic_Day. For more photos ping me jack@photoking.com :)\"'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete all characters other than digits, alphabetical characters, and whitespaces from the text. Use the split() function to split the strings into parts."
      ],
      "metadata": {
        "id": "vbW5HSgZUx6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAW8LRjHUzRN",
        "outputId": "2c9b2774-d9fc-48af-9e50-a6696ccbee7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted',\n",
              " 'Witnessing',\n",
              " '90th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'official',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'official',\n",
              " 'VietNam',\n",
              " '90thRepublic',\n",
              " 'Day',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack',\n",
              " 'photoking',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, extracting each token separately does not help. For instance, consider the sentence, \"I don't hate you, but your behavior.\" Here, if we process each of the tokens, such as \"hate\" and \"behavior,\" separately, then the true meaning of the sentence would not be comprehended. In this case, the context in which these tokens are present becomes essential. Thus, we consider n consecutive tokens at a time. n-grams refers to the grouping of n consecutive tokens together."
      ],
      "metadata": {
        "id": "whlPH1_MVXgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extract n-grams by using nltk and textBlb"
      ],
      "metadata": {
        "id": "DMYViGL-Ve2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram_extractor(sentence, n):\n",
        "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        print(tokens[i:i+n])"
      ],
      "metadata": {
        "id": "YOu9TI1AVkXO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the bi-grams we need to pass the function with text and n\n",
        "n_gram_extractor('The cute little boy is playing with balls.', 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfbnqzHIWDtF",
        "outputId": "0c2ac699-689a-4a06-fe35-c82414ff9b7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cute']\n",
            "['cute', 'little']\n",
            "['little', 'boy']\n",
            "['boy', 'is']\n",
            "['is', 'playing']\n",
            "['playing', 'with']\n",
            "['with', 'balls']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the tri-grams we need to pass the function with text and n\n",
        "n_gram_extractor('The cute little boy is playing with balls.', 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGix-6fJWUUa",
        "outputId": "69dc231b-d044-4d73-9fca-54f40e8a4173"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cute', 'little']\n",
            "['cute', 'little', 'boy']\n",
            "['little', 'boy', 'is']\n",
            "['boy', 'is', 'playing']\n",
            "['is', 'playing', 'with']\n",
            "['playing', 'with', 'balls']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The cute little boy is playing with balls.\")\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJte-kLEWeil",
        "outputId": "7c59928a-69f7-43f3-a31a-5977776c402d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute']),\n",
              " WordList(['cute', 'little']),\n",
              " WordList(['little', 'boy']),\n",
              " WordList(['boy', 'is']),\n",
              " WordList(['is', 'playing']),\n",
              " WordList(['playing', 'with']),\n",
              " WordList(['with', 'balls'])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The cute little boy is playing with balls.\")\n",
        "blob.ngrams(n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J2XVSiLW33L",
        "outputId": "8ba861a3-3843-4a78-c6e9-e24c13ab047e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute', 'little']),\n",
              " WordList(['cute', 'little', 'boy']),\n",
              " WordList(['little', 'boy', 'is']),\n",
              " WordList(['boy', 'is', 'playing']),\n",
              " WordList(['is', 'playing', 'with']),\n",
              " WordList(['playing', 'with', 'balls'])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras and TextBlob are two of the most popular Python libraries used for performing various NLP tasks. TextBlob provides a simple and easy-to-use interface to do so. Keras is used mainly for performing deep learning-based NLP tasks"
      ],
      "metadata": {
        "id": "qSApLCg9XLc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to perform tokenization with keras and textblob"
      ],
      "metadata": {
        "id": "kapdCNYeXzb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "a_Gtx4OGX5MO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_word_sequence(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0chhwTgYBXe",
        "outputId": "7b7c1843-24cf-41bf-db21-3f03735ae6b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happy',\n",
              " 'tweeted',\n",
              " 'witnessing',\n",
              " '90th',\n",
              " 'republic',\n",
              " 'day',\n",
              " 'of',\n",
              " 'viet',\n",
              " 'nam',\n",
              " 'from',\n",
              " 'jack',\n",
              " 'ho',\n",
              " 'chi',\n",
              " 'minh',\n",
              " 'mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'vietnamese',\n",
              " 'army',\n",
              " 'awesome',\n",
              " 'airshow',\n",
              " 'viet',\n",
              " 'nam',\n",
              " 'official',\n",
              " 'viet',\n",
              " 'nam',\n",
              " 'official',\n",
              " 'vietnam',\n",
              " '90threpublic',\n",
              " 'day',\n",
              " 'for',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack',\n",
              " 'photoking',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzA3vlpYKNx",
        "outputId": "b35a2d3e-6dd2-4cd5-fcd1-327c4a54225c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Happy', 'tweeted', 'Witnessing', '90th', 'Republic', 'Day', 'of', 'Viet', 'Nam', 'from', 'Jack', 'Ho', 'CHi', 'Minh', 'Mesmerizing', 'performance', 'by', 'Vietnamese', 'Army', 'Awesome', 'airshow', 'Viet_Nam_official', 'Viet_Nam_official', 'VietNam', '90thRepublic_Day', 'For', 'more', 'photos', 'ping', 'me', 'jack', 'photoking.com'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet tokenizer: This is specifically designed for tokenizing tweets. It is\n",
        "capable of dealing with emotions and expressions of sentiment, which are used widely on Twitter.\n",
        "\n",
        "MWE tokenizer: MWE stands for Multi-Word Expression. Here, certain groups of multiple words are treated as one entity during tokenization, such as \"United States of America,\" \"People's Republic of China,\" \"not only,\" and \"but also.\"\n",
        "\n",
        "\n",
        "Regular expression tokenizer: These tokenizers are developed using regular expressions. Sentences are split based on the occurrence of a particular pattern.\n",
        "\n",
        "Whitespace tokenizer: This tokenizer splits a string whenever a space, tab, or newline character is present.\n",
        "\n",
        "Word Punkt tokenizer : This splits a text into a list of alphabetical characters, digits, and non-alphabetical characters.\n"
      ],
      "metadata": {
        "id": "4iBGWGKCZEfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QA3CPaZkg_",
        "outputId": "49dd6878-c3d8-4e92-f5b7-c04dee8d6eaa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted',\n",
              " ',',\n",
              " '\"',\n",
              " 'Witnessing',\n",
              " '90th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack',\n",
              " ',',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh',\n",
              " '.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army',\n",
              " '!',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " '!',\n",
              " '@Viet_Nam_official',\n",
              " '@Viet_Nam_official',\n",
              " '#VietNam',\n",
              " '#90thRepublic_Day',\n",
              " '.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack@photoking.com',\n",
              " ':)',\n",
              " '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Republic', 'Day')])\n",
        "mwe_tokenizer.add_mwe(('Vietnamese', 'Army'))\n",
        "mwe_tokenizer.tokenize(sentence.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMiIJs8gZ9xR",
        "outputId": "8e9a180e-112d-4b1e-9e24-c9a963be6041"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted,',\n",
              " '\"Witnessing',\n",
              " '90th',\n",
              " 'Republic_Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack,',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army!',\n",
              " 'Awesome',\n",
              " 'airshow!',\n",
              " '@Viet_Nam_official',\n",
              " '@Viet_Nam_official',\n",
              " '#VietNam',\n",
              " '#90thRepublic_Day.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack@photoking.com',\n",
              " ':)\"']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above result, the words \"Indian\" and \"Army!\" were supposed to be treated as a single identity, but they got separated. This is because \"Army!\" (not \"Army\") is treated as a token."
      ],
      "metadata": {
        "id": "7Q50YT27asTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mwe_tokenizer.tokenize(sentence.replace('!', '').split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igXE_mQoaw22",
        "outputId": "0b116e7f-3293-4146-925a-400eec979bb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted,',\n",
              " '\"Witnessing',\n",
              " '90th',\n",
              " 'Republic_Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack,',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese_Army',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " '@Viet_Nam_official',\n",
              " '@Viet_Nam_official',\n",
              " '#VietNam',\n",
              " '#90thRepublic_Day.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack@photoking.com',\n",
              " ':)\"']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "reg_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYrlpchza_Vp",
        "outputId": "99a6f87a-091e-4313-d226-e43cfc2ef2c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted',\n",
              " ',',\n",
              " '\"Witnessing',\n",
              " '90th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack',\n",
              " ',',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh',\n",
              " '.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army',\n",
              " '!',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " '!',\n",
              " '@Viet_Nam_official',\n",
              " '@Viet_Nam_official',\n",
              " '#VietNam',\n",
              " '#90thRepublic_Day.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack',\n",
              " '@photoking.com',\n",
              " ':)\"']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "wh_tokenizer = WhitespaceTokenizer()\n",
        "wh_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLu-6LwCbWMz",
        "outputId": "6c8e7fcc-9fd6-4ea6-d009-83b1eeee0bb3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted,',\n",
              " '\"Witnessing',\n",
              " '90th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack,',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army!',\n",
              " 'Awesome',\n",
              " 'airshow!',\n",
              " '@Viet_Nam_official',\n",
              " '@Viet_Nam_official',\n",
              " '#VietNam',\n",
              " '#90thRepublic_Day.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack@photoking.com',\n",
              " ':)\"']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wp_tokenizer = WordPunctTokenizer()\n",
        "wp_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APvw2-8TbmtU",
        "outputId": "c3cfc10c-a093-452b-c273-090fdb3fdbfb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Happy',\n",
              " 'tweeted',\n",
              " ',',\n",
              " '\"',\n",
              " 'Witnessing',\n",
              " '90th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'Viet',\n",
              " 'Nam',\n",
              " 'from',\n",
              " 'Jack',\n",
              " ',',\n",
              " 'Ho',\n",
              " 'CHi',\n",
              " 'Minh',\n",
              " '.',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Vietnamese',\n",
              " 'Army',\n",
              " '!',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " '!',\n",
              " '@',\n",
              " 'Viet_Nam_official',\n",
              " '@',\n",
              " 'Viet_Nam_official',\n",
              " '#',\n",
              " 'VietNam',\n",
              " '#',\n",
              " '90thRepublic_Day',\n",
              " '.',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'jack',\n",
              " '@',\n",
              " 'photoking',\n",
              " '.',\n",
              " 'com',\n",
              " ':)\"']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although tokenization appears to be an easy task, in reality, it is not so. This is primarily because of ambiguities that arise due to the presence of whitespaces and hyphens. Moreover, sentences in certain languages, such as Chinese and Japanese, do not have words separated by whitespaces, thus making it difficult to tokenize them."
      ],
      "metadata": {
        "id": "7qzmXGaXcFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = TextBlob('She sells seashells on the seashore')"
      ],
      "metadata": {
        "id": "vmwl_eJ-aUC9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will check the lost of words in sentence\n",
        "sentence1.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFUmeBtEahpQ",
        "outputId": "9e0f9143-5fd8-4b35-ec66-7abd1391469e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['She', 'sells', 'seashells', 'on', 'the', 'seashore'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It is a time to extract words from a sentence using TextBlob\n",
        "sentence1.words[2].singularize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wIHsDvqXarTK",
        "outputId": "30c76f0e-c758-433f-98e3-4f00eb2d711c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashell'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will pluralize the fifth word\n",
        "sentence1.words[5].singularize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VuY74_tJa8KY",
        "outputId": "ade30b28-1fc7-4418-cff2-eed13be95f65"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the TextBlb to translate a sentence from Spanish to Engish"
      ],
      "metadata": {
        "id": "1mTPHB7ybJyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_blob = TextBlob(u'muy bien')\n",
        "en_blob.translate(from_lang='es', to='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APC-Sz04bH20",
        "outputId": "41fbda9b-169e-4c65-db49-4ba32e17b439"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"very good\")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Feature extraction in natural language processing (NLP) refers to the process of transforming raw text data into a format that is suitable for machine learning algorithms to understand and analyze. In NLP tasks, such as text classification or sentiment analysis, it's crucial to represent text data in a numerical format that algorithms can work with effectively. Feature extraction involves identifying and extracting relevant information or features from text data to create structured numerical representations.\n",
        "\n",
        "Common techniques for feature extraction in NLP include:\n",
        "\n",
        "Bag of Words (BoW): This approach represents text as a collection of words, ignoring grammar and word order. Each document is represented as a vector where each dimension corresponds to a unique word in the corpus, and the value indicates the frequency of that word in the document.\n",
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF): TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a corpus. It takes into account the frequency of a word in a document (TF) and inversely scales it by the frequency of the word across the entire corpus (IDF).\n",
        "\n",
        "Word Embeddings: Word embeddings represent words as dense vectors in a continuous vector space, where words with similar meanings are located close to each other. Techniques like Word2Vec, GloVe, and fastText are popular methods for generating word embeddings.\n",
        "\n",
        "N-grams: N-grams are contiguous sequences of n items from a given text, typically words or characters. They capture contextual information by considering sequences of words rather than individual words alone.\n",
        "\n",
        "Part-of-Speech (POS) Tagging: POS tagging assigns a grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence. These tags can serve as features for various NLP tasks.\n",
        "\n",
        "Named Entity Recognition (NER): NER identifies and classifies named entities (such as names of people, organizations, locations, etc.) within text documents. Extracted named entities can be used as features in tasks like information extraction or document summarization.\n",
        "\n",
        "Syntactic Parsing: Syntactic parsing involves analyzing the grammatical structure of sentences. Features derived from parsing trees, such as dependency relations or syntactic constituents, can provide valuable information for various NLP tasks.\n",
        "\n",
        "Feature extraction is a crucial step in NLP pipelines as it enables machine learning algorithms to effectively process and analyze textual data, ultimately facilitating tasks like sentiment analysis, document classification, machine translation, and more."
      ],
      "metadata": {
        "id": "gz7w7OhYeDmO"
      }
    }
  ]
}