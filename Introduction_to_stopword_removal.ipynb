{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal is a text preprocessing technique in Natural language processing (NLP) that involves eliminating common words known as stopwords from a pice of text. Stopwords are words that are very frequent in a langauge but often contribute little to overall meaning or context of the text. Examples of stopwords in English include \"the\",\"and\",\"is\",\"in\",\"to\", \"off\" and so on\n",
        "\n",
        "The process of stopword removal aims to reduce the dimensionality of the text data and improve the efficiency and effectiveness of language processing tasks. By removing stopwrds, NLP models can focus on the more meaningful and contextually relevant words, resulting in more accurate and meaningful analysis of the text.\n",
        "\n",
        "Stopword removal is often performed as one of the initial steps in NLP along with other preprocessing tasks like tokenization, lowercasing and lemmatization or stemming"
      ],
      "metadata": {
        "id": "R4BcgwXhN5Ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s3nMxbs6Ne8s"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog\""
      ],
      "metadata": {
        "id": "6C1Uh3__PA_a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the list of stopwords from NLTK\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvDmqDnEPHMl",
        "outputId": "389acff9-70ff-44c4-af05-81a9281b89d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "T247bAJFPKL3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ejgoV8WPYyX",
        "outputId": "ef557ba8-39d1-4356-fbee-15285a4179a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "Z761Q3H_PQwi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
      ],
      "metadata": {
        "id": "mplhXNOyPWTR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the filterd tokens back into sentence\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsYB0nXhPkDy",
        "outputId": "0dfcb21f-5c0f-4fb7-dbac-0af1c7bfe7da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quick brown fox jumps lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the texample above, we used NLTK's list of English stopwords to remove common words from the sample text. After tokenizing the text, we filtered out the stopwords, resulting in a more concise represetnation of the text\n",
        "\n",
        "\n",
        "Stopword remoal is especially useful in tasks such as text classificaition m sentiment analysis and information retrieval where common words can introduce noise and hinder the accurate interpretation of the text. By excluding stopwords, NLP models can focus on the essital context and meaningful patterns leadning to improved performance in various langauge processing applications."
      ],
      "metadata": {
        "id": "JPlHC4itQVuv"
      }
    }
  ]
}