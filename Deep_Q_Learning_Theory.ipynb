{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q Learning or using deep Q-networks is considered one of the most mordern reinforcement learning technique.We will develop various deep Q-network models step by step and apply them to solve several reinforcement learning problems. We will start with vanilla Q-networks and enahnce them with experience reply. We will also improve robustness by using an additional target networka dn demonstrate how to fine-tune a Deep Q-Network. We will also experiment with dueling deep Q-network and see how their value functions differ from other types of deep Q-Network.\n",
        "\n",
        "In Deep Q-learning, a neural network is trained to output the appropriate Q(s,a_ values for each action given the input state, s. The action a, of the agent is chosen based on the ouput Q(s,a ) values following the epsilon-greedy policy\n",
        "\n",
        "Steps to develop deep Q-learing using DQN to solve the Mountain Car:\n",
        "\n",
        "\n",
        "1.   Import all the necessary packages.\n",
        "\n",
        "1.   Let's start with the __init__method of the DQN class.\n",
        "\n",
        "1.   We now develop the training method which updates the neural network with a data point\n",
        "\n",
        "1.   Next is the prediction of the state value for each action given a state\n",
        "\n",
        "1.   We will begin by creating a Mountain Car environment\n",
        "2.   We will define the epsilon-greedy policy\n",
        "2.   Now, we will define the deep Q-learining algorithm with DQN\n",
        "2.   We then specify the size of the hidden layer and the learning rate and create a DQN instance accordingly.\n",
        "2.   We then perform Deep Q-learning with the DQN we just develop for 1000 episodes and also keep track of the total (original) rewards for each episode\n",
        "2.   Display the plot of the episode reward overtime.\n",
        "\n"
      ],
      "metadata": {
        "id": "rNtHp1ol1id7"
      }
    }
  ]
}