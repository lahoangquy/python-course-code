{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbaZvb2tpOcD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing (NLP), a \"bag of words\" (BoW) is a common representation technique used to analyze and represent text data. It simplifies the text into an unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word. The term \"bag\" implies that the words are treated as individual and isoalted elements, much like items in a bg, without considering the order in which they appear.\n",
        "\n",
        "Below are the steps explain  how the bag of words model works\n",
        "\n",
        "1. Tokenization: The first step is to break down a piece of text into individual words or tokens. This process involves removing punctuation and splitting the text into words.\n",
        "\n",
        "2. Vocabulary creation: Create a vocabulary containing all unique words present in the entire corpus (collection of documents). Each word in the vocabulary is assigned a unique index.\n",
        "\n",
        "3. Vectorization: Represent each document in the corpus as a vector. The ector has the same legnth as the vocabulary and each position corresponds to a word in the vocabulary. The value at each position indicates the frequency of the corresponding word in the document."
      ],
      "metadata": {
        "id": "hKWCcIQdv5XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKlX5zLgxeGT",
        "outputId": "7fb5e18b-21e1-4887-826d-dbd2722b68a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are learning about Natural Language Processing\", \"Natural Language Processing helps computers understand language data\", \"The field of Natural Language Processing is evolving everyday\"]"
      ],
      "metadata": {
        "id": "4Zz8pe4myUUh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a pandas series object from the list of sentences\n",
        "corpus = pd.Series(sentences)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTlIydQSytYT",
        "outputId": "6cd5d541-1470-4011-f862-88a7801788e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are learning about Natural Language Processing\n",
              "1    Natural Language Processing helps computers un...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "eZS11SVH0VYT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "ouIEgiyV0TaV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "LOzdoEbHzr7j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "65wQX6Vbzww_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "vUM579cZz2d8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "Y6PB4TRr0JHH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus using the NLP pipline.\n",
        "\n",
        "preprocessed_corpus = preprocess(corpus, \\\n",
        "    keep_list = common_dot_words, stemming = False, \\\n",
        "    stem_type = None, lemmatization = True, \\\n",
        "    remove_stopwords = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsQHkZFGy-Ef",
        "outputId": "7f9af6e3-7267-418d-9a4b-705f8fed8ce0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a7d5d0d93ac1>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-4-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-4-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-4-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqgFUvn1pk2",
        "outputId": "e5974d08-105c-45dc-c83a-8e7fbc72baf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['learn natural language process',\n",
              " 'natural language process help computers understand language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build our vocabulary\n",
        "set_of_words = set()\n",
        "for sentence in preprocessed_corpus:\n",
        "  for word in sentence.split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkmSDD2Y10nC",
        "outputId": "00deddfe-fb44-42fd-828c-ef0ef7d40f11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['evolve', 'computers', 'field', 'everyday', 'data', 'help', 'language', 'learn', 'process', 'natural', 'understand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the position/indexx of each token in the vocabulary\n",
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "  position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpNKcTCC2KUc",
        "outputId": "5dd77597-a9ef-4670-8b58-416d6c4dc00d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'evolve': 0, 'computers': 1, 'field': 2, 'everyday': 3, 'data': 4, 'help': 5, 'language': 6, 'learn': 7, 'process': 8, 'natural': 9, 'understand': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a placeholder matrix for holding the BoW.\n",
        "# The shape of the matrix is (number of sentences * length of vocabulary)\n",
        "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
      ],
      "metadata": {
        "id": "kXRB-aAD2eZx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code creates a matrix called bow_matrix which is a full of zeros in the shape ((len(preprocessed_corpus), len(vocab))) using the numpy libarry. preprocessed_corpus and vocab are both probably lists or array. The built-in len function gets the number of elements in those lists or arrays.preprocessed_corpus might be a list of text data that has already been preprocessed (eg converted into a standard format, any noise like punctuation or irrelevant symbols have beeen remove). Vocab is probably a list of distinct vocabulary words int eh corpus. The result is a mtrix (a 2-dimentional array) with as many rows as there are lem ents in preprocessed_corppus and as many columns as there are elements in vocab. This sort of matrix could be used for many various things in natural language processing, but one common use is as a \"Bag of words\" matrix. In a Bag of words model, each row of the matrix corresponds to a docuament or senteice, each column corresponds to a particuar words int the vocabulary and the entry in the matrix at position (i,j) tells that the occurence of word j in document i (like its frequency of occurence or its TF-IDF score, etc)."
      ],
      "metadata": {
        "id": "-4RrG3w-5cqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase the positional index of every owrd by 1 if it appears in  a sentence\n",
        "\n",
        "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "    for token in preprocessed_sentence.split():\n",
        "        bow_matrix[i][position[token]] = \\\n",
        "                            bow_matrix[i][position[token]] + 1"
      ],
      "metadata": {
        "id": "dw2faLrR2y9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag of words model represents each document or sentence as a vector in an m-dimentional coordinate space where m is the number of various tokens, e.g words in a corpus. The order of tokens is disregarded, hence the term bag. The code above, is peforming the below operations\n",
        "1. For i , preprocessed_sentence in enumerate(preprocessed corpus), it will iterate over each setence.\n",
        "\n",
        "2. For toekn in preprocessed_sentence.split(), it will tokenize each sentence in to separate words using the split function, which default splits at white spaces.\n",
        "\n",
        "3. bow_matrix[i][position[token]] + . If a token (word) is found, it woll increase the count in its corresponding position int eh BoW matrix. It will assume that bow_matrix is a 2D list (or similar construct) and position is a dictionary that maps each token to a specific index. The final BoW matrix stores a count of the number of times each word appears in each setnece, learding to a comprehensive , although crude and context-free, representatiuon of the text data of the entire corpus."
      ],
      "metadata": {
        "id": "RnmZ7Har3tu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQPoqjyP5Nhn",
        "outputId": "4893ade2-cdfe-4af7-9edd-502cf6142415"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}