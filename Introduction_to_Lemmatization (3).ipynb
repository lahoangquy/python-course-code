{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a word normalization technique used to convert words into their base or dictionary form known as the lemma. The purpose of lemmatization is to reduce inflected or derived words to their cononical form, which is an actual word found in the language's vocabulary. Lemmatization helps to group words with the smae root meaning together, enabling more accurate and meaningful language analysis.\n",
        "\n",
        "Unlike stemming, which simply cuts off prefixes or suffixes to obtain a word's base form, lemmatization uses a lexicon or dictionary to map words to their correspoiding lemmas. This ensures that the resulting base form is a valid word in the language and retians its acutal meaning.\n",
        "\n",
        "Word: running Lemma : \"run\"\n",
        "Word: jumps Lemma : \"jump\"\n",
        "Word: fishes Lemma : \"fish\"\n",
        "Word: happiness Lemma : \"happiness\"\n",
        "Word: better Lemma : \"good\"\n",
        "\n",
        "As can be seen from the above examples, lemmatization produces meaningful adn valid base forms, making it more accurate and contextually appropriate than stemming. By converting words to their lemmas, lemmatization ensures that variations of a word such as plural forms, verb tenses, or adjectival forms are correctly mapped to their common base form.\n",
        "\n",
        "\n",
        "Lemmatization is especially useful in NLP tasks taht require a deeper understanding of the langauge such as text classification , sentiment analysis, and langauge modeling. It also helps to reduce the vocabulary size and improve the efficientcy of lanauge processing tasks, similar to stemming. However, lemmatization can be computationally more expensive than stemming as it requires access to a lexicon or dictionary to perform the mapping acucrately. Despite this, lemmatization is a valuable techniqe for mainting the integrity and meaningfullness of words in NLP applications."
      ],
      "metadata": {
        "id": "UeIKhZnsuE58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wordnet lemmatizer is a lemmatization tool available in Natural language toolkit (NLTK) library in Python. It is based on WordNet which is a large lexical database of Enlish words and their semantic relationships. The WordNet lemmatizer uses WordNet's lexicon to convert words into thier base or lemma forms, ensuring that the resulting base form is a vlid word in the English Language\n",
        "\n",
        "WordNet lemmatizer is a part of NLTK's WordNet module which provides access to WordNet's extensive collection of words, synonymns, antonyms and semantic relationships. The lemmatizer utinizes the synsets (synonym sets) and morphological information soted in wordNet to perform accurate lemmatization of words.\n",
        "\n",
        "Unlike other lemmatization techniques that might require complex rules and laogirhms to map words to their lemmas, The WordNet Lemmatizer provides a straightforward and relinable way to perform lemmatization using WordNet's pre-existing knowledge."
      ],
      "metadata": {
        "id": "TvLZQhi48Hfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are putting in efforts to enhance our understanding of Lemmatization"
      ],
      "metadata": {
        "id": "UO12CGz-9lkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "wGUUPQji9mNY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--HVCkaW9sLs",
        "outputId": "9ecb8c41-7b5d-458f-851d-8a0ecd7a52df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "s= \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "token_list = s.split()\n",
        "print(\"The tokens are: \", token_list)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
        "print(\"The lemmatized output is: \", lemmatized_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t6sV_sM9wX7",
        "outputId": "65254730-6462-4c24-9cc2-c46e71146300"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, the WordNet lemmatizer did not do much ere. Out of are, putting, efforts and understanding, non were converted to their base form\n",
        "\n",
        "The WordNet lemmatizer works well if the POS tags are also provided as inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "x2no-nOX-aWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybFL9t3i-4Id",
        "outputId": "9845018e-7527-4ebf-abd7-8813ca92a726"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(token_list)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqqeXU-l_GAI",
        "outputId": "1609b49d-78fd-4bde-ae4d-9b3f9b25bfd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('putting', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('efforts', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enhance', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('understanding', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Lemmatization', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, a list of tuples of the form (the otken  and POS tag) is returned by the POS tagger. Now the POST tags need to be converted to a form that can be understodd by the WorddNet lemmatizer and sent in as input along with the tokens"
      ],
      "metadata": {
        "id": "4BzUlHhE_LzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "q_dkn3NB_e7V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_part_of_speech_tags(token):\n",
        "\n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "4rFj0vkw_i9p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
        "print(' '.join(lemmatized_output_with_POS_information))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWitUFxAAiNP",
        "outputId": "f4dd9010-e93d-47cb-e624-6215891e9539"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We be put in effort to enhance our understand of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it converts are to be, putting to put, efforts to effort and understanding to understand"
      ],
      "metadata": {
        "id": "KZANsZhrA5fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using snowball stemmer.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer2 = SnowballStemmer(language='english')\n",
        "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
        "print(' '.join(stemmed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1fNo5fXBAoH",
        "outputId": "5497b1bb-8e44-41ca-fcd9-074f2b72be76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are put in effort to enhanc our understand of lemmat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, the WordNet lemmatizer makes a sensible and context-aware conversion of the token into its base form, unlike the semmer which tries to chop the affixes from the token."
      ],
      "metadata": {
        "id": "rnwkmrY1BW9V"
      }
    }
  ]
}