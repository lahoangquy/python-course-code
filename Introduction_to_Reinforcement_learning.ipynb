{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Recently, Reinforcement learning (RL) has been gaining more and more popularity. Notably, many of its break thorugh have come from improvements from supervised methods such as deep learning. At the moment, most RL algorithms are used in virtual environments such as video games. Luckily there are many companies such as OpenAI have created and released learning environments where it is easy to test the algorithm in various environment.\n",
        "\n",
        "It is possible to download this learning environment called Gym from OpenAI's website.\n",
        "\n",
        "Additionally there are real-world applications on RL and some of them are incredibly impactful. For example, DeepMind, after beign used to optimize Google's data centers was able to reduce the energy consumption and overall energy bill of Gooogle's data centers by 10% and 40%.\n",
        "\n",
        "A major problem in these algorithms is generalizing the learning. Ultimately we want to solve real-world problems but the most efficient way of exploring RL algorithms is by applying it to video games.\n",
        "\n",
        "There are 2 main ways to make sure that the algorithm will be successfully applied to real-world problems:\n",
        "\n",
        "\n",
        "\n",
        "1.   Focusing the simulation: In this way we want to make sure that the environment we are running is generic enough so that we will maintain similar results when switching to the real-world application. This is viable only for very specific applications as the cost of reproducing accurately complex environments increases with their complexity.\n",
        "2.   Focusing the algorithm. if we follow this one, we will need to ensure that the algorithm is generic enough to be successfully applied to the real-world applications. This approach is easier to implement for complex problems compared to the previous method as it is possible to test the algorithm in more virual envirn=onments ot see how it performs in various scenarios.\n",
        "\n",
        "\n",
        "Some of the basic definitions:\n",
        "\n",
        "\n",
        "1.   Agent: The agent is whatever the algorithm has control over and receives feedback from. If we think about robotics, clearly the agent is the robot that needs to perform a certain action. In the case of video games (for example, Pong), the agent is only the pad of the algorithm controls.\n",
        "2.   The environment is everything that is not agent and the one thing that the algorithms can not control directly. This is a broad definition and sometimes quite difficult to define the environment in practice. To prove that we will talk about Pong game. This game uses a very simple environment but we still have some options in terms of what to consider as the environment. For instance should we consider every pixel on the screen as a part of our environemtn? what about the score and delotted line in the middle? More complex examples can be a robot which will require a lot more thinking as we want to find the most complete and simple representation to guide our algorithm to the best solution.\n",
        "1.   Goal and feedback: The Goal can be defined as what our algorithms wants to achieve and this feedback is what is provided to the algorithm to guide it in the right direction. One of the most important differentiators of RL from other methods is that RL does not optimize to the next action but to the cumulative sum of all rewards. Together with the definition of the environment this is the most critical step in defining our problem. For example , Pong, the goal is that we want to win the game, but there are multiple ways in which this can be achieved. Let's assume, for example that we think our agent should be rewarded when it does not allow the opponent to score a point. We could provide some positive feedback after ny second it has passed, but in this case, the agent will try to neither win nor lose the game as that will limit its reward. Instead, it will try to keep the game runnin g as long as possible by avoiding scoring or receiving points.\n",
        "1.   State is how we represent the current situation of the environment. in video games, a popular representation is to have the screen in raw pixel values as if it was an iamge, this allows us to use some common computer vision techniques in particular deep learning. For a robot, the state could be represented by join angles and volocities.\n",
        "2.   Action space. This refers to all the possible action our agent can take. It is usually environment specific and can be discrete or continuous. This distinction is quite important because some algorithms are specific to either continous or a discrete action space.\n",
        "1.   Policy: This refers to a set of rules that the agent has learned so that it can decide which action to take to maximize the total reward. Policies are usually denoted with π. There are 2 various types of policies: Deterministic and stochastic. A deterministic policy is a well-defined ampping from space to action. The formula is : π: S -> A.\n",
        "\n",
        "A stochastic policy will sample froma  distribution that uses the state as a priority to decide what action to take. π(a|s)\n",
        "\n",
        "Stochastic policies are what deep neural entworks (DNNs) are used for. They have the advantage of sampling the action and conputing the likelihood of particular actions. With sampling,  we are exploring alternatives that are not maximizing our goals with the current assumptions which is one of the main advantages of RL. By computing the maximum likelihood we are maximizing our gains which is another key aspect of RL.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7XdSqvdSrusA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G4fXDADq6pM"
      },
      "outputs": [],
      "source": []
    }
  ]
}