{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The tweetTokenizer is a specialized tokenizer used in Natural Language processing (NLP) to handle text data from social media platforms, particularly tweets from Twitter. It is a part of the Natural Lanugage Toolkits (NLTK) library in Python and is designed to address the unique characteristics and chllenges present in tweets such as hastags, mentions, emoticons and internet slang.\n",
        "\n",
        "Tweets often contain non-standard langauge, abbreviations and specific twitter-related elements making traditional tokenizers less effective for such data. The TweetTokenizer is specifically tailored to capture these characteristics and provide more accurate tokenizations for tweets.\n",
        "\n",
        "\n",
        "There are several key features of TweetTokenizer:\n",
        "\n",
        "\n",
        "\n",
        "*   Handling Emoticons: The TweetTokenizer is capable of recognizing and preserving emoticons which are commonly used in tweets to express emotions. For example it can tokenize \":)\", \":-)\" or \"XD\" as seperate tokens.\n",
        "\n",
        "*   Retaining Hashtags: Hashtags (#) are prevalent in tweets to categorize or tag content. The tokenizer treats hastags as individual toens to maintain their significance. For example, \"#NLP\" would be tokenized as [\"#NLP\"]\n",
        "\n",
        "*   Capturing mentions: Mentions (@) are used to reference other twitter uses in tweets . The tokenizer preserves mentions as seperate tokens. For example \"@username\" would be tokenized as [\"@username\"]\n",
        "*   Addressing Internet Slang: The tweettokenizer can handle internet slang and abbreviations often used in tweets ensuring that they are properly tokenized.\n",
        "\n"
      ],
      "metadata": {
        "id": "jx-TE0o9_4IZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmFYAphp_VoZ",
        "outputId": "45833396-1e50-43c2-c4c3-0e483bcea919"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@amankedia',\n",
              " \"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another common thing with social media writing is the use of expressions such as Rolexxxxxxxxx. Here a lot of x's are present in addition to the normal one. It is a common trend and should be addressed to bring tit to a form as close to nornal as possible.\n",
        "\n",
        "\n",
        "The tweetTokenizer provide 2 additional parameters in the form of reduce_len which tries to reduce the excessive characters in a token."
      ],
      "metadata": {
        "id": "4LF6YBbJDpx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNJIJBxEMEq",
        "outputId": "d7898b0b-bad4-4e9a-aa2f-ec7eaaa42d02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, the word Rolexxxx is actually tokenized as Rolexxx in an attempt to reucce the number of x's present\n",
        "\n",
        "The parameter strip_hanles when set to True, removes the handles mentioned in a post/tweet.@amankedia is stripped since it is handled.\n",
        "\n",
        "\n",
        "One more parameter that is available with TweetToeknizer is preserve_case, which when set to Flase, converts everyhtong to lower case to normalize the vocabulary. The default value for this parameter is True."
      ],
      "metadata": {
        "id": "hVwy00t0EUNI"
      }
    }
  ]
}