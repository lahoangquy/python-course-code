{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKpnATPKnA5j",
        "outputId": "4b6fb3df-d6ae-4445-efb4-572b566e5dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adjustText\n",
            "  Downloading adjustText-0.8-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from adjustText) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from adjustText) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n",
            "Installing collected packages: adjustText\n",
            "Successfully installed adjustText-0.8\n"
          ]
        }
      ],
      "source": [
        "pip install adjustText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from adjustText import adjust_text"
      ],
      "metadata": {
        "id": "mYfXIUgrnGcF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWNL5RgfnYWN",
        "outputId": "4a0e861d-17b3-4157-e8b4-35f23b103080"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw0j1X7jnZzN",
        "outputId": "111c0bff-537f-48cd-c709-e01d369f5673"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'"
      ],
      "metadata": {
        "id": "ij29yeYMnb6Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we will use BBC news articles dataset. It contains 2225 news articles belonging to 5 topics, business, entertainment, politics, sport, and tech which were published on the BBC website between 2004-2005"
      ],
      "metadata": {
        "id": "84sy2nXAptpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url, data_dir):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print('Downloading file...')\n",
        "        filename, _ = urlretrieve(url, file_path)\n",
        "    else:\n",
        "        print(\"File already exists\")\n",
        "\n",
        "    extract_path = os.path.join(data_dir, 'bbc')\n",
        "    if not os.path.exists(extract_path):\n",
        "\n",
        "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
        "            zipf.extractall(data_dir)\n",
        "\n",
        "    else:\n",
        "        print(\"bbc-fulltext.zip has already been extracted\")\n",
        "\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7bX7dhBp9rg",
        "outputId": "6035c53d-5266-48d5-ea1f-fdc93dc0c552"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists\n",
            "bbc-fulltext.zip has already been extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function first creates data_dir if it does not exst. Next if the bbc-fulltext.zip file does not exist it will be downloaded from the URL. If bbc-fulltext.zip has not been extracted yet, it will be extracted to data_dir"
      ],
      "metadata": {
        "id": "_aszykr3rakW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that we are going to focus on reading the data contained in the news articles (in .txt format) into the memory. To do that we will define the read_data() function which takes a data directory path (data_dir) and reads the .txt files (except for README file)"
      ],
      "metadata": {
        "id": "05NlTCM5rvVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "\n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "\n",
        "    print(\"Reading files\")\n",
        "\n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "\n",
        "        for fi, f in enumerate(files):\n",
        "\n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "\n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "\n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "\n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "\n",
        "                    story.append(row.strip())\n",
        "\n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)\n",
        "                # Add that to the list\n",
        "                news_stories.append(story)\n",
        "\n",
        "        print('', end='\\r')\n",
        "\n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories"
      ],
      "metadata": {
        "id": "wYKxcyEhsFaD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with read_data() function. We can use it read the data and print some samples as well as some statistics"
      ],
      "metadata": {
        "id": "vO93yj7Bs-k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VNmIs5vtE2K",
        "outputId": "6326ef51-8a53-4ef2-ca43-4232d695fa1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 284.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Musicians to tackle US red tape  Musicians' groups\n",
            "Example words (end):  illion songs downloaded since it launched in 2003.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above result, there are 2225 stories with close to a milion words. In the next step we will tokenize each story (in the form of a long string) to a list of tokens (or words). Along with that we will perform some preprocessing on the text: lowercase all the chracfters and remove punctuation."
      ],
      "metadata": {
        "id": "x8mvPASdtnNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' '\n",
        ")"
      ],
      "metadata": {
        "id": "OWUcMagc5s4_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen above some of the most popular keywrd arugment and their default values used when defining a tokenizer:\n",
        "\n",
        "\n",
        "\n",
        "*   num_words: Deifnines the size of the vocabulary. Defaults to None, meaning it will consider all the words appearing in the text corpus. If set to the integer n, it will only consider the n most common words appearing in the corpus\n",
        "\n",
        "*   filters: Defines any characters that need to be omitted during preprocessing. By default it defines a string containing most of the common punctuation marks and symbols.\n",
        "\n",
        "*   lower: defines whether the text needs to be converted to lowercase\n",
        "\n",
        "*   split: defines the character that the words will be tokenized on.\n",
        "\n",
        "\n",
        "Once the tokenizer is defined, fit_on_texts() emthod will be call with a list of strings (where each string is a news article) so that the tokenizer will learn the vocabulary and map the words to the unique IDs\n",
        "\n"
      ],
      "metadata": {
        "id": "mN2ITBbp-nXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(news_stories)"
      ],
      "metadata": {
        "id": "FZ7RctIA_kJ0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a time to take a moment to analyze what the tokenizer has produced after it has been fitted on the text. Once it has been fitted, the Tokenizer will have 2 important attributes populated: word_index and index_word. Here word_index is a dictionary that maps each word to a unique ID. The index_word attribute is the opposite of word_index, that is a dictionary that maps each unique word ID to the corressponding word"
      ],
      "metadata": {
        "id": "TR4Q5B_4_rqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(tokenizer.word_index.items()) + 1\n",
        "print(f\"Vocabulary size: {n_vocab}\")\n",
        "print(\"\\nWords at the top\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
        "print(\"\\nWords at the bottom\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQXwddNC_rWe",
        "outputId": "68e4c03c-9beb-4a1b-fad1-325f5b00ad29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32360\n",
            "\n",
            "Words at the top\n",
            "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
            "\n",
            "Words at the bottom\n",
            "\t {\"taipei's\": 32350, 'taller': 32351, 'petronas': 32352, 'skyscraper': 32353, 'packaged': 32354, 'inserting': 32355, 'solves': 32356, 'idefence': 32357, 'pls': 32358, 'm3u': 32359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how we are using the length of the word_index dictionary to derive the vocabulary size. We need an additional 1 as the ID 0 is a reserved ID and will not be used for any word.\n",
        "\n",
        "The more frequent a word in the corpus, the lower the ID will be. Words such as \"the\", \"to\",  and \"of\" which tend to be common (and are called stop words) are infact the most common words. As the next step , we are going to refine our tokenizer object to have a limited-size vocabulary. Because we are working with a relatively small corpus, we have to ensure the vocabulary is not too large as it can lead to poorly learned word vectors due to the lack of data."
      ],
      "metadata": {
        "id": "2nZ24m0MA1yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = 15000\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab-1,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', oov_token='',\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02ASLstmBvDI",
        "outputId": "8a28cf28-aa2a-4cce-f37e-a0130b1b4fce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have a total vocabulary of more than 30000 words, we will restrict the eize of the vocabulary to 15000. This means the tokenizer will only keep the most common 15000 words as the vocabulary. When we restrict a vocabulary this way a new problem arises. As the tokenizer's vocabulary does not encompass all possible words in the true vocabulary, out-of-vocabulary words (or OOV words) can rear their heads. Some soluations are to replace OOV words with a special token (such  as <UNK>) or remove them from the corpus. This is possible by passing the string we want to replace OOV tokens to the oov_token argument in the Tokenizer. In this case, we will remove OOV words. If we are careful when setting the size of the vocabulary, omitting some of the rare words would not harn learning the context of words accurately\n",
        "\n",
        "It is time to convert a string of the first 100 cahracters to the first story in our corpos (stored in the news_stories variable)"
      ],
      "metadata": {
        "id": "rWlvazLGCDO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original: {news_stories[0][:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMHbVz2HFWBd",
        "outputId": "c57db9dc-311a-4b83-965b-ac3a4dcc37e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Musicians to tackle US red tape  Musicians' groups are to tackle US visa regulations which are blame\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can call the tokenizer's texts_to_sequence() method to convert a list of documents (where each document is a string) to a list of list of word IDs (that is , each domuent is converted to a list of word IDs)"
      ],
      "metadata": {
        "id": "sBtIlcMvFd6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYtMC0zDF6qt",
        "outputId": "7ed468ef-ddbb-49a9-b2d6-81c7ebf688bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence IDs: [2609, 3, 1342, 49, 1294, 4686, 11185, 862, 25, 3, 1342, 49, 2529, 3218, 35, 25, 3696]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have our tikenizer sorted. There is nothing left to do but to convert all of our news articles to sequences of word IDs with a signle line of code\n"
      ],
      "metadata": {
        "id": "UIa5VPcAGJFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ],
      "metadata": {
        "id": "54lnzOgtGSxU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word_ids = news_sequences[0][:5]\n",
        "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
        "print(f\"Sample phrase: {sample_phrase}\")\n",
        "print(f\"Sample word IDs: {sample_word_ids}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhWX5lx3GcPM",
        "outputId": "25bdfc1e-0540-4977-ed55-cde634fded54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample phrase: musicians to tackle us red\n",
            "Sample word IDs: [2609, 3, 1342, 49, 1294]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is a time to consider a window size of 1. This means for a given target word, we define the context as one word frim each side of the target word"
      ],
      "metadata": {
        "id": "LHFJQhumG53f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 1 # How many words to consider left and right"
      ],
      "metadata": {
        "id": "_z4sCTZ4HCIJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have all ingredients to define extract skip-grams from the sample phrase."
      ],
      "metadata": {
        "id": "RiLMvgFrHHsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=n_vocab,\n",
        "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample skip-grams\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgGS-YkzHGkh",
        "outputId": "930b3b7e-ffbb-443c-b795-f6aef67ecd0e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample skip-grams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is a time to look at some of important arguments which have been used.\n",
        "\n",
        "\n",
        "*   sequence(list[str] or list[int]) : A list of words or word IDS\n",
        "\n",
        "*   Vocabulary_size (int):  Size of the vocabulary\n",
        "\n",
        "*   window_size (int): size of the window to be considered for the context\n",
        "\n",
        "*   window_size : defines the length on each side\n",
        "\n",
        "*   Nevative_samples (int): Fraction of negative candidates to generate. For example, a value of 1 means that there will be an equal number of possitive and negative skipgram candidates. A value of 0 means there will not be any negative candiates\n",
        "*   Shuffle (bool): Whetehre to produce labels as categorical (that is one-hot encoded) or integers\n",
        "\n",
        "\n",
        "*   Sampling_table : An arry size of the same size as the vocabulary. An element is a given position to the array represents the probability of sampleing the word indexed by taht position in the tokenizer's word ID to word mapping\n",
        "\n",
        "\n",
        "\n",
        "*   seed(int). If shuffling is enabled, this is the random seed to be used for shuffling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--3DKvkrH2xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With the inputs and labels generated. It is time to print some data\n",
        "print(\"Sample skip-grams\")\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NURZffHGtnh",
        "outputId": "d547626b-9fbd-4a5c-ed71-7f4fa98e534d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample skip-grams\n",
            "\tInput: [2609, 3] (['musicians', 'to']) / Label: 1\n",
            "\tInput: [3, 2609] (['to', 'musicians']) / Label: 1\n",
            "\tInput: [3, 1342] (['to', 'tackle']) / Label: 1\n",
            "\tInput: [1342, 3] (['tackle', 'to']) / Label: 1\n",
            "\tInput: [1342, 49] (['tackle', 'us']) / Label: 1\n",
            "\tInput: [49, 1342] (['us', 'tackle']) / Label: 1\n",
            "\tInput: [49, 1294] (['us', 'red']) / Label: 1\n",
            "\tInput: [1294, 49] (['red', 'us']) / Label: 1\n",
            "\tInput: [3, 2732] (['to', 'forum']) / Label: 0\n",
            "\tInput: [3, 9965] (['to', 'larry']) / Label: 0\n",
            "\tInput: [49, 1569] (['us', 'spokeswoman']) / Label: 0\n",
            "\tInput: [1294, 13245] (['red', 'tyler']) / Label: 0\n",
            "\tInput: [1342, 11710] (['tackle', 'gangsters']) / Label: 0\n",
            "\tInput: [49, 7694] (['us', 'belonging']) / Label: 0\n",
            "\tInput: [1342, 11420] (['tackle', 'penned']) / Label: 0\n",
            "\tInput: [2609, 8421] (['musicians', 'michel']) / Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, since the word \"sales\" appears in the context of the word \"ad\", it is considered a positive candidate. On the other hand, since the word \"racing\" (randomly sampled from the vocabulary) does not appear in the context of the word \"ad\". It is added as a negative candidate.\n",
        "\n",
        "When selecting negative candiates, the skipgram() function selects them randomly, giving uniform weights to all the words in the vocabulary. However, the original paper exapliants that this can lead to poor performance. A better strategy is to use the unigram distribution as a prior for selecting negative context words.\n",
        "\n",
        "a unigram distribution referes to the frequency distribution of individual words in a text or a language corpus. In simpler terms, it represetns how often each distinct word appears in a given body of text.\n",
        "\n",
        "A unigram is the smalles lunguistic unit, representing a single word without considering its context or enighboring words. Analyzing the unigram distribution can provide insights into the vocabulary richness, word usage patterns and potentially even the topic or theme of the text. This distribution is often used as a foundational step in various language processing tasks, such as langauge modeling, text classificaiton and information retrival.\n",
        "\n",
        "To do that we can use tf.random.log_uniform_candiate_sampler() function. This function will take a batch of positive context candiates of shape [b,num_true], where b is the batch size and num_true is the number of true candiates per example (1 for the skip-gram model), and it outputs a [num_sampled] size array where num_sampled is the number of negative samples we need."
      ],
      "metadata": {
        "id": "HfvRnzS8GsYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
        "    window_size=window_size, negative_samples=0, shuffle=False,\n",
        ")\n",
        "\n",
        "inputs, labels = np.array(inputs), np.array(labels)"
      ],
      "metadata": {
        "id": "E2lfZK8WJqmI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note taht we are specifying negative_samples = 0, as we will be generating negative samples with the candiate ssampler. Now it is a time to use the tf.random.log_uniform_candidate_sampler() function to generate negative candidates."
      ],
      "metadata": {
        "id": "o7FTj9lBKOns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
        "    # A true context word that appears in the context of the target\n",
        "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
        "    num_true=1, # number of true words per example\n",
        "    num_sampled=10,\n",
        "    unique=True,\n",
        "    range_max=n_vocab,\n",
        "    name=\"negative_sampling\"\n",
        ")"
      ],
      "metadata": {
        "id": "9jTe8D40Kgk2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes some arguments:\n",
        "\n",
        "\n",
        "1.   true_classes(np.ndarray or tf.Tensor) - A tensor containing true target words. This needs to be a [b, num_true] sized array, where num_true denotes the number of true context candidates per example. Since we have 1 context word per example this is 1\n",
        "\n",
        "1.   num_true (int): The number of true context erms per example\n",
        "\n",
        "1.   num_sampled (int): The number of negative samples to generate\n",
        "\n",
        "1.   unique (bool): whether to genrate unique samples with replacement\n",
        "2.   range_max (int): The size of the vocabulary\n",
        "\n",
        "\n",
        "It will return\n",
        "\n",
        "2.   sampled_candiates(tf.tensor) :  A tensor of size [num_sampled] containing negative candiates\n",
        "\n",
        "2.   true_expected_count(tf.tensor): A tensor of size[b, num_true]: The probability of each true candidate being sampled\n",
        "\n",
        "\n",
        "2.   Sampled_expected count (tf.tensor): A tensor of size [num_sapled]: The probabilities of each negative sample occuring alon g with true candiates. if sampled from corpus\n",
        "\n"
      ],
      "metadata": {
        "id": "Tyk3inhqLHJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
        "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
        "print(f\"true_expected_count: {true_expected_count}\")\n",
        "print(f\"sampled_expected_count: {sampled_expected_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2kggkh3MjVd",
        "outputId": "d06edf2c-f543-4e51-db2d-6dceeea0eb5e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample: [[3]]\n",
            "Negative samples: [    9    30     6    56     1    78   592  3898 13259     4]\n",
            "true_expected_count: [[0.22761466]]\n",
            "sampled_expected_count: [1.0378349e-01 3.5725005e-02 1.4257596e-01 1.9716201e-02 3.7742513e-01\n",
            " 1.4295651e-02 1.9257633e-03 2.9331696e-04 8.6263499e-05 1.8987593e-01]\n"
          ]
        }
      ]
    }
  ]
}